[
{
	"uri": "/post/",
	"title": "Announcements",
	"tags": [],
	"description": "",
	"content": " Colors\nUpon grading the labs I noticed there is interest in adding color to the plots. I understand and appreciate the desire to make your figures more aesthetically pleasing. I have used colors to make the figures fit the overall style of whatever publication they are going to be used in. However, my general advice is to only use color when it adds value. For example, coloring bars or points based upon a quantitative variable is often very helpful.\n  Random Numbers and Seeds in R\nTo explore how R generates randome numbers, we will use the rnorm function. This function draws a random number from a normal distribution with a mean = 0 and standard deviation = 1 (though these can be changed with the mean and sd parameters). With n = 1 we will get two random numbers. rnorm(n = 1) ## [1] 1.733789 rnorm(n = 1) ## [1] -1.516682 Each time you run the command you will get a different number.\n  Getting Started with R\nTo get started with the labs, you will need to install R and other supporting applications. Links to download all the software is on the Software page at http://data606.net/software which is available on the Course Overview menu. Once you are in RStudio, you will need to install a number of R packages using the following two commands: install.packages(c(\u0026#39;openintro\u0026#39;,\u0026#39;OIdata\u0026#39;,\u0026#39;devtools\u0026#39;,\u0026#39;tidyverse\u0026#39;, \u0026#39;ggplot2\u0026#39;, \u0026#39;psych\u0026#39;,\u0026#39;reshape2\u0026#39;,\u0026#39;knitr\u0026#39;,\u0026#39;markdown\u0026#39;,\u0026#39;shiny\u0026#39;,\u0026#39;R.rsp\u0026#39;, \u0026#39;fivethirtyeight\u0026#39;)) devtools::install_github(\u0026quot;jbryer/DATA606\u0026quot;) Note that this course used to be listed as IS 606, hence the different package name.\n  Welcome to DATA606!\nWelcome to DATA606! My name is Dr. Jason Bryer and I will be your instructor for this semester. I am an Assistant Professor in the Data Science and Information Systems department at CUNY SPS. Couple of important notes as you get started: The course syllabus located here: https://fall2020.data606.net I will post all course materials there. Blackboard will be used primarily for submitting assignments. Join the Slack channel by clicking here.\n  "
},
{
	"uri": "/chapters/chapter1/",
	"title": "Chapter 1",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Introduction to Data Learning Objectives  Identify the type of variables (e.g. numerical or categorical; discrete or continuous; ordered or not ordered). Identify the relationship between multiple variables (i.e. independent vs. dependent). Define variables that are not associated as independent. Be able to describe and identify the difference between observational and experimental studies. Distinguish between simple random, stratified, and cluster sampling, and recognize the benefits and drawbacks of choosing one sampling scheme over another. Identify the four principles of experimental design and recognize their purposes: control any possible con- founders, randomize into treatment and control groups, replicate by using a sufficiently large sample or repeating the experiment, and block any variables that might influence the response.  Supplemental Readings  OpenIntro Statistics slides  Videos OpenIntro provides a number of videos. You may find these helpful while reading the chapter.\nCase Study: Using Stents to Prevent Strokes\n Data Basics: Observations, Variable, and Data Matrices\n Data Collection Principles\n Observational Studies and Sampling Strategies\n Designing Experiments\n [Using Randomization to Analyze a Gender Discrimination Study](Using Randomization to Analyze a Gender Discrimination Study)\n "
},
{
	"uri": "/assignments/daacs/",
	"title": "DAACS",
	"tags": [],
	"description": "",
	"content": "The Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, I encourage you to review the resources recommended to you. We will use the aggregated results in class. To get credit for this assignment:\n Go to demo.daacs.net Create an account. Complete the Self-Regulated Learning (SRL) assessment. Enter the email address you used in step 2 in Blackboard.  "
},
{
	"uri": "/course-overview/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "Schedule",
	"content": " Note: Schedule is subject to change. Last updated November 18, 2020 09:47PM.\nClick here to import the course calendar into your calendar application\n    Start End Topic    Wednesday, August 26, 2020 Sunday, August 30, 2020 Chapter 1 - Intro to Data  Monday, August 31, 2020 Sunday, September 06, 2020 Chapter 2 - Summarizing Data  Monday, September 07, 2020 Sunday, September 13, 2020 Chapter 3 - Probability  Monday, September 14, 2020 Sunday, September 27, 2020 Chapter 4 - Distributions  Monday, September 28, 2020 Sunday, October 04, 2020 Chapter 5 - Foundation for Inference  Monday, October 05, 2020 Sunday, October 11, 2020 Chapter 6 - Inference for Categorical Data  Monday, October 12, 2020 Sunday, October 18, 2020 Chapter 7 - Inference for Numerical Data  Monday, October 19, 2020 Sunday, November 01, 2020 Chapter 8 - Linear Regression  Monday, November 02, 2020 Sunday, November 29, 2020 Chapter 9 - Multiple and Logistic Regression  Monday, November 30, 2020 Sunday, December 06, 2020 Intro to Bayesian Analysis  Wednesday, December 09, 2020 Sunday, December 13, 2020 Final Exam    "
},
{
	"uri": "/chapters/chapter2/",
	"title": "Chapter 2",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Summarizing Data Learning Outcomes  Use appropriate visualizations for different types of data (e.g. histogram, barplot, scatterplot, boxplot, etc.). Use different measures of center and spread and be able to describe the robustness of different statistics. Describe the shape of distributions vis-a-vis histograms and boxplots. Create and intepret contingency and frequency tables (one- and two-way tables).  Supplemental Readings  OpenIntro Statistics slides ggplot2 - ggplot2 is an R package by Wickham that implements the grammer of graphics (Wilkinson, 2005) in R. I will frequently make use of the graphing framework throughout the course and is worth learning. Visualizing Likert Data - An R package for visualizing Likert scale data built on the ggplot2 framework. Quick-R base graphics - Covers many of the visualizations using R\u0026rsquo;s base graphics.  Videos Summarizing and Graphing Numerical Data\n Exploring Categorical Data\n Note about Pie Charts There is only one pie chart in OpenIntro Statistics (Diez, Barr, \u0026amp; ??etinkaya-Rundel, 2015, p. 48). Consider the following three pie charts that represent the preference of five different colors. Is there a difference between the three pie charts? This is probably a difficult to answer.\nHowever, consider the bar plot below. Here, we cleary see there is a difference between the ratio of the three colors. As John Tukey famously said:\n There is no data that can be displayed in a pie chart that cannot better be displayed in some other type of chart\n Source: https://en.wikipedia.org/wiki/Pie_chart.\n"
},
{
	"uri": "/course-overview/",
	"title": "Course Overview",
	"tags": [],
	"description": "",
	"content": " Schedule\nSchedule\n  Meetups\nMeetups\n  Textbooks\nRequired Diez, D.M., Barr, C.D., \u0026amp; Çetinkaya-Rundel, M. (2019). OpenIntro Statistics (4th Ed). This is an open source textbook and can be downloaded in PDF format here, from the OpenIntro website, or a printed copy can be ordered from Amazon. Navarro, D. (2018, version 0.6). Learning Statistics with R This is free textbook that supplements a lot of the material covered in Diez and Barr. We will use the chapter on Bayesian analysis.\n  Software\nComputer Hardware This course will make extensive use of the R statistical language. You are expected to have a computer sufficient to run this the software listed below. The software will run on most platforms so a Mac or PC (running Windows or Linux) will work fine. I recommend having at least 16GBs of RAM. R and RStudio We will make use of R, an open source statistics program and language.\n  Links\nThese are some useful resources on the web for learning R. Feel free to suggest other resources by clicking the \u0026ldquo;Improve this page\u0026rdquo; button in the top right. Learning R R for Data Science. Book by Garrett Grolemund and Hadley Wickham Quick-R. Kabakoff\u0026rsquo;s website. Great reference along with his book, R in Action. O\u0026rsquo;Reilly Try R. Great tutorial on R where you can try R commands directly from the web browser.\n  Math Equations\nOccasionally you will need to type equations in homework and labs. R Markdown supports LaTeX style equations using the MathJax javascript library. I do not expect you to learn LaTeX for this course. Instead, I recommend using the free application Daum Equation Editor. It availabe online, as a Google Chrome Extension, or as a standalone Mac Application. Creating Equations with Daum Equation Editor Occasionally you will need to type equations in homework and labs.\n  Materials\nMaterials\n  "
},
{
	"uri": "/course-overview/meetups/",
	"title": "Meetups",
	"tags": [],
	"description": "Meetups",
	"content": "  There will be weekly meetups. You are encouraged to attend as many as you can but recordings will generally be availabe the day after the meetup.\nJoin Zoom Meeting: https://albany.zoom.us/j/93697731603\nMeeting ID: 936 9773 1603\nOne tap mobile: +16465588656,,93697731603# US (New York)\nPhone Number: +1 646-558-8656 US US (New York)\nPlease note: Students who participate in this class with their camera on or use a profile image are agreeing to have their video or image recorded solely for the purpose of creating a record for students enrolled in the class to refer to, including those enrolled students who are unable to attend live. If you are unwilling to consent to have your profile or video image recorded, be sure to keep your camera off and do not use a profile image. Likewise, students who un-mute during class and participate orally are agreeing to have their voices recorded. If you are not willing to consent to have your voice recorded during class, you will need to keep your mute button activated and communicate exclusively using the “chat” feature, which allows students to type questions and comments live.\nPresentation Signup Sheet\n  Date  StartTime  Topic  Resources      Wed, Aug 26, 2020  8:30 pm  Intro to Course / Intro to Data  Video, Intro Course Slides, Intro to Data Slides    Wed, Sep 02, 2020  8:30 pm  Summarizing Data  Video, Slides    Wed, Sep 09, 2020  8:30 pm  Probability  Video, Slides    Wed, Sep 16, 2020  8:30 pm  Distributions Part I  Video, Slides, Is the “hot hand” real?    Wed, Sep 23, 2020  8:30 pm  Distributions Part II  Video, Slides    Wed, Sep 30, 2020  8:30 pm  Foundation for Inference  Video, Slides    Wed, Oct 07, 2020  8:30 pm  Inference for Categorical Data  Video, Slides, Chi-Squared Distribution App    Wed, Oct 14, 2020  8:30 pm  Inference for Numerical Data  Video, Slides, t-Distribution App, F-Distribution App    Wed, Oct 21, 2020  8:30 pm  Linear Regression  Video, Slides, Shiny App, Signficance-vs-Sample-Size.R, Bootstrap_and_Permutations.R    Wed, Oct 28, 2020  8:30 pm  Linear Regression  Video, Slides    Wed, Nov 04, 2020  8:00 pm  Project Proposal Discussion  Video, Slides, Rmd file for slides    Wed, Nov 11, 2020  8:30 pm  Multiple Regression  Video, Slides, Order_of_Predictors.R    Wed, Nov 18, 2020  8:30 pm  Logistic Regression  Video, Slides    Wed, Nov 25, 2020   NO CLASS - Thanksgiving     Wed, Dec 02, 2020  8:30 pm  Intro to Bayesian Analysis     Wed, Dec 09, 2020  8:30pm  Wrap-Up      "
},
{
	"uri": "/assignments/presentation/",
	"title": "Presentation",
	"tags": [],
	"description": "",
	"content": "You are responsible for presenting one practice homework problem during the semester. Please sign up as-soon-as possible as there are limited slots per week. Try to keep each presentation to five minutes or less. Sign-up on this Google Spreadsheet: https://docs.google.com/spreadsheets/d/14OFGJZFkiI6JM_wn8bl-_BsKlbLybdgR5IDVrsk5HfE/edit?usp=sharing\nChoose a problem from the following list:\n Chapter 1 - Introduction to Data  Practice: Any odd numbered question.   Chapter 2 - Summarizing Data  Practice: Any odd numbered question.   Chapter 3 - Probability  Practice: Any odd numbered question.   Chapter 4 - Distributions of Random Variables  Practice: Any odd numbered question.   Chapter 5 - Foundations for Inference  Practice: Any odd numbered question.   Chapter 6 - Inference for Categorical Data  Practice: Any odd numbered question.   Chapter 7 - Inference for Numerical Data  Practice: Any odd numbered question.   Chapter 8 - Introduction to Linear Regression  Practice: Any odd numbered question.   Chapter 9 - Multiple and Logistic Regression  Practice: Any odd numbered question.    "
},
{
	"uri": "/chapters/chapter3/",
	"title": "Chapter 3",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Probability Learning Outcomes  Define trial, outcome, and sample space. Define and describe the law of large numbers. Distinguish disjoint (also called mutually exclusive) and independent events. Use Venn diagrams to represent events and their probabilities. Describe probability distributions. Distinguish between marginal and conditional probabilities. Use tree diagrams and/or Bayes Theorem to calculate conditional probabilities and probabilities of intersection of non-independent events. The expected value of a discrete random variable is computed by adding each outcome weighted by its probability.\n$$ E(X)=\\mu=\\sum_{i=1}^{k}{{x}_{i}P\\left(X={x}_{i}\\right)} $$ The variance of a discrete random variable is computed by adding each squared deviation of an outcome from the expected value weighted by its probability. The standard deviation is the square root of the variance.\n$$ Var(X)={\\sigma}^{2}=\\sum_{i=1}^{k}{{\\left({x}_{i}-\\mu\\right)}P\\left(X={x}_{i}\\right) } $$ The average of a linear combination of discrete random variables is computed as the sum of their averages, weighted by the constant multipliers. The variance of a linear combination of independent discrete random variables is computed as the sum of their variances, weighted by the square of the constant multipliers. The distribution of a continuous random variable is described by the probability density function. The total area under the density curve is 1. Probabilities under the density curve can be calculated as the area under the curve. The probability of a continuous random variable being exactly equal to a value is 0, since there is no area under the curve at a given location.  Supplemental Readings   OpenIntro Statistics slides\n  Matloff, N. (2009). From Algorithms to Z-Scores: Probabilistic and Statistical Modeling in Computer Science. Available from http://heather.cs.ucdavis.edu/probstatbook.\n  Videos Probability Introduction\n Would You Take This Bet?\n The Monty Hall Problem\n "
},
{
	"uri": "/assignments/",
	"title": "Assignments",
	"tags": [],
	"description": "",
	"content": " DAACS\nThe Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, I encourage you to review the resources recommended to you.\n  Presentation\nYou are responsible for presenting one practice homework problem during the semester. Please sign up as-soon-as possible as there are limited slots per week. Try to keep each presentation to five minutes or less. Sign-up on this Google Spreadsheet: https://docs.google.com/spreadsheets/d/14OFGJZFkiI6JM_wn8bl-_BsKlbLybdgR5IDVrsk5HfE/edit?usp=sharing Choose a problem from the following list: Chapter 1 - Introduction to Data Practice: Any odd numbered question. Chapter 2 - Summarizing Data Practice: Any odd numbered question.\n  Homework\nThe solutions to the practice problems are at the end of the book and do not need to be handed in. Graded assignments should use the provided R markdown templates provided below. Data for the homework assignments, and for within the chapters too, can be downloaded here. Or alternatively all the data is included in the openintro R packge (use the data(package = 'openintro') command to list all the datasets available in that package).\n  Labs\nThese mini projects will have you explore statistical topics using R. You can use the startLab function in the DATA606 package to get started, or copy the templates from the links below. Please submit a PDF (preferred) or HTML file along with your Rmarkdown file. Be sure to answer all questions in lab, not just the on your own section. Labs should be submitted on Blackboard. Introduction to R and RStudio (Template) Introduction to Data (Template) Probability (Template) Distributions of Random Variables (Template) Foundations for Statistical Inference Sampling Distributions (Template) Confidence Levels (Template) Inference for Categorical Data (Template) Inference for Numerical Data (Template) Introduction to Linear Regression (Template) Multiple Linear Regerssion (Template)   Project\nDownload project proposal template Download project template The purpose of the data project is for you to conduct a reproducible analysis with a data set of your choosing. There are two components to the project, the proposal, which will be graded on a pass/fail basis, and the final report. The outline for each of these are provided in the templates. When submitting the assignments, include the R Markdown file (change the name to include your last name, for example Bryer-Proposal.\n  Final Exam\nThe final exam will be posted on Blackboard during the time indicated on the course schedule.\n  "
},
{
	"uri": "/assignments/homework/",
	"title": "Homework",
	"tags": [],
	"description": "",
	"content": "The solutions to the practice problems are at the end of the book and do not need to be handed in. Graded assignments should use the provided R markdown templates provided below. Data for the homework assignments, and for within the chapters too, can be downloaded here. Or alternatively all the data is included in the openintro R packge (use the data(package = 'openintro') command to list all the datasets available in that package). Right click on the \u0026ldquo;Template\u0026rdquo; link and choose \u0026ldquo;Save link as\u0026hellip;\u0026rdquo; to save the R markdown file to your computer. By default, the Rmarkdown files should generate PDFs. This is the preferred format since PDFs can be uploaded to Blackboard. See the software course page for instructions on installing LaTeX.\n Chapter 1 - Introduction to Data (Template) Chapter 2 - Summarizing Data (Template) Chapter 3 - Probability (Template) Chapter 4 - Distributions of Random Variables (Template) Chapter 5 - Foundations for Inference (Template) Chapter 6 - Inference for Categorical Data (Template) Chapter 7 - Inference for Numerical Data (Template) Chapter 8 - Introduction to Linear Regression (Template) Chapter 9 - Multiple and Logistic Regression (Template)  "
},
{
	"uri": "/course-overview/textbooks/",
	"title": "Textbooks",
	"tags": [],
	"description": "",
	"content": "Required Diez, D.M., Barr, C.D., \u0026amp; Çetinkaya-Rundel, M. (2019). OpenIntro Statistics (4th Ed).\n This is an open source textbook and can be downloaded in PDF format here, from the OpenIntro website, or a printed copy can be ordered from Amazon.\n Navarro, D. (2018, version 0.6). Learning Statistics with R\n This is free textbook that supplements a lot of the material covered in Diez and Barr. We will use the chapter on Bayesian analysis. You can download a PDF version, Bookdown version, or visit the author\u0026rsquo;s website at learningstatisticswithr.com.\n  Recommended Wickham, H., \u0026amp; Grolemund, G. (2016) R for Data Science. O\u0026rsquo;Reilly.\n Most of this books is available freely online at r4ds.had.co.nz/ but can be purchased from Amazon.\n Wickham, H. Advanced R. Baca Raton, FL: Taylor \u0026amp; Francis Group.\n Most of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.\n Kruschke, J.K. (2014). Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (2nd Ed). London: Academic Press.\n This book can be purchased from Amazon, but also check out the author\u0026rsquo;s webiste (doingbayesiandataanalysis.blogspot.com/) for additional resources.\n "
},
{
	"uri": "/chapters/chapter4/",
	"title": "Chapter 4",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Distributions of Random Variables Learning Outcomes  Define the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: $Z = \\frac{x - \\mu}{\\sigma}$. Use the Z score  if the distribution is normal: to determine the percentile score of a data point (using technology or normal probability tables) regardless of the shape of the distribution: to assess whether or not the particular observation is considered to be unusual (more than 2 standard deviations away from the mean)   Depending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score. Assess whether or not a distribution is nearly normal using the 68-95-99.7% rule or graphical methods such as a normal probability plot.  Reading: Section 4.1 of OpenIntro Statistics Test yourself: True/False: In a right skewed distribution the Z score of the median is positive.   If X is a random variable that takes the value 1 with probability of success $p$ and 0 with probability of success $1-p$, then $X$ is a Bernoulli random variable. The geometric distribution is used to describe how many trials it takes to observe a success. Define the probability of finding the first success in the $n^{th}$ trial as $(1-p)^{n-1}p$.  $\\mu = \\frac{1}{p}$ $\\sigma^2 = \\frac{1-p}{p^2}$ $\\sigma = \\sqrt{\\frac{1-p}{p^2}}$   Determine if a random variable is binomial using the four conditions:  The trials are independent. The number of trials, n, is fixed. Each trial outcome can be classified as a success or failure. The probability of a success, p, is the same for each trial.   Calculate the number of possible scenarios for obtaining $k$ successes in $n$ trials using the choose function: ${n \\choose k} = \\frac{n!}{k!~(n - k)!}$. Calculate probability of a given number of successes in a given number of trials using the binomial distribution: $P(k = K) = \\frac{n!}{k!~(n - k)!}~p^k~(1-p)^{(n - k)}$. Calculate the expected number of successes in a given number of binomial trials $(\\mu = np)$ and its standard deviation $(\\sigma = \\sqrt{np(1-p)})$. When number of trials is sufficiently large ($np \\ge 10$ and $n(1-p) \\ge 10$), use normal approximation to calculate binomial probabilities, and explain why this approach works.  Supplemental Readings  OpenIntro Statistics slides  Videos     "
},
{
	"uri": "/assignments/labs/",
	"title": "Labs",
	"tags": [],
	"description": "",
	"content": "These mini projects will have you explore statistical topics using R. You can use the startLab function in the DATA606 package to get started, or copy the templates from the links below. Please submit a PDF (preferred) or HTML file along with your Rmarkdown file. Be sure to answer all questions in lab, not just the on your own section. Labs should be submitted on Blackboard.\nIntroduction to R and RStudio (Template) Introduction to Data (Template) Probability (Template) Distributions of Random Variables (Template) Foundations for Statistical Inference Sampling Distributions (Template) Confidence Levels (Template)  Inference for Categorical Data (Template) Inference for Numerical Data (Template) Introduction to Linear Regression (Template) Multiple Linear Regerssion (Template)  "
},
{
	"uri": "/course-overview/software/",
	"title": "Software",
	"tags": [],
	"description": "",
	"content": "Computer Hardware This course will make extensive use of the R statistical language. You are expected to have a computer sufficient to run this the software listed below. The software will run on most platforms so a Mac or PC (running Windows or Linux) will work fine. I recommend having at least 16GBs of RAM.\nR and RStudio We will make use of R, an open source statistics program and language. Be sure to install R and RStudio on your own computers within the first few days of the class.\n R - Windows or Mac RStudio - Download Windows or Mac version from here  If using Windows, you also need to download RTools and ActivePerl.\nLaTeX LaTeX is a typesetting language for preparing documents. Documents are written in plain text files. Formatting the document is done using specific markup. If you have used HTML, the framework is similar however instead of using \u0026lt;TAG\u0026gt;\u0026lt;/TAG\u0026gt; syntax, LaTeX uses \\TAG{} format. We will primarily use Markdown, and its extension R Markdown for preparing documents in this class. However, when preparing PDF documents, the Markdown will first be converted to LaTeX before creating the PDF file. As such, a LaTeX converter is necessary. There are LaTeX installers for Windows (MiKTeX) and Mac (BasicTeX). Alternatively, the tinytex R package provides an easier way of installing LaTeX directly from within R:\ninstall.packages('tinytex') tinytex::install_tinytex() Source Control All course materials will be made available on Github which provides an implementation of the git open source version control system. RStudio supports git directly, but I recommend downloading Sourcetree. This is a free desktop client that provides an easier interface for working with Github. You will also need to create an account on Github.\nFor more information, Jenny Bryan\u0026rsquo;s Happy Git and Github for the useR is a free online book covering the important features of source control for R users.\nR Packages Once everything is installed, execute the following command in RStudio to install the packages we will use for this class (you can copy-and-paste):\ninstall.packages(c('openintro','OIdata','devtools','tidyverse', 'ggplot2', 'psych','reshape2','knitr','markdown','shiny','R.rsp', 'fivethirtyeight')) devtools::install_github(\u0026quot;jbryer/DATA606\u0026quot;) The DATA606 R Package Many of the course resouces are available in the DATA606 R package. Here are some command to get started:\nlibrary('DATA606') # Load the package vignette(package='DATA606') # Lists vignettes in the DATA606 package vignette('os4') # Loads a PDF of the OpenIntro Statistics book data(package='DATA606') # Lists data available in the package getLabs() # Returns a list of the available labs viewLab('Lab1') # Opens Lab1 in the default web browser startLab('Lab1') # Starts Lab1 (copies to getwd()), opens the Rmd file shiny_demo() # Lists available Shiny apps "
},
{
	"uri": "/chapters/chapter5/",
	"title": "Chapter 5",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Foundations for Inference Learning Outcomes  Define sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous. Recognize that point estimates (such as the sample proportion) will vary from one sample to another, and define this variability as sampling variation. Calculate the sampling variability of the proportion, the standard error, as $SE = \\sqrt{\\frac{p(1-p)}{n}}$, where $p$ is the population proportion.  Note that when the population proportion $p$ is not known (almost always), this can be estimated using the sample proportion, $SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$.   Standard error measures the variability in point estimates from different samples of the same size and from the same population, i.e. measures the sampling variability. Recognize that when the sample size increases we would expect the sampling variability to decrease.  Conceptually: Imagine taking many samples from the population. When sample sizes are large the sample proportion will be much more consistent across samples than when the sample sizes are small. Mathematically: $SE = ???$, when $n$ increases, $SE$ will decrease since $n$ is in the denominator.   Notice that sampling distributions of point estimates coming from samples that don\u0026rsquo;t meet the required conditions for the CLT (about sample size and independence) will not be normal. Define a confidence interval as the plausible range of values for a population parameter. Define the confidence level as the percentage of random samples which yield confidence intervals that capture the true population parameter. Calculate an approximate 95% confidence interval by adding and subtracting 2 standard errors to the point estimate: $point~estimate \\pm 2 \\times SE$. Recognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.  In the case of the proportion the CLT tells us that if  the observations in the sample are independent, and there are at least 10 successes and 10 failures, \\end{itemize} then the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of $\\sqrt{\\frac{p(1-p)}{n}}$. [ \\hat{p} \\sim N \\left( mean = p, SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right) ]   When the population proportion is unknown, condition (2) can be checked using the sample proportion.   Recall that independence of observations in a sample is provided by random sampling (in the case of observational studies) or random assignment (in the case of experiments).  In addition, the sample should not be \\textit{too} large compared to the population, or more precisely, should be smaller than 10% of the population, since samples that are too large will likely contain observations that are not independent. \\end{itemize}   Recognize that the nearly normal distribution of the point estimate (as suggested by the CLT) implies that a more precise confidence interval can be calculated as [ point~estimate \\pm z^{\\star} \\times SE, ] where $z^{\\star}$ corresponds to the cutoff points in the standard normal distribution to capture the middle XX% of the data, where XX% is the desired confidence level.  For proportions this is $\\bar{x} \\pm Z^\\star \\sqrt{\\frac{p(1-p)}{n}}$. Note that $z^{\\star}$ is always positive.   Define margin of error as the distance required to travel in either direction away from the point estimate when constructing a confidence interval, i.e. $z^{\\star} \\times SE$.  Notice that this corresponds to half the width of the confidence interval.   Interpret a confidence interval as \u0026ldquo;We are XX% confident that the true population parameter is in this interval\u0026rdquo;, where XX% is the desired confidence level.  Note that your interpretation must always be in context of the data \u0026ndash; mention what the population is and what the parameter is (mean or proportion).   Explain how the hypothesis testing framework resembles a court trial. Recognize that in hypothesis testing we evaluate two competing claims:  the null hypothesis, which represents a skeptical perspective or the status quo, and the alternative hypothesis, which represents an alternative under consideration and is often represented by a range of possible parameter values.   Construction of hypotheses:  Always construct hypotheses about population parameters (e.g. population proportion, $p$) and not the sample statistics (e.g. sample proportion, $\\hat{p}$). Note that the population parameter is unknown while the sample statistic is measured using the observed data and hence there is no point in hypothesizing about it. Define the null value as the value the parameter is set to equal in the null hypothesis. Note that the alternative hypothesis might be one-sided ($\\mu$ $\u0026lt;$ or $\u0026gt;$ the null value) or two-sided ($\\mu$ $\\ne$ the null value), and the choice depends on the research question.   Define a p-value as the conditional probability of obtaining a sample statistic at least as extreme as the one observed given that the null hypothesis is true. $\\text{p-value} = \\text{P(observed or more extreme sample statistic}~|~H_0 \\text{ true)}$ Calculate a p-value as the area under the normal curve beyond the observed sample proportion (either in one tail or both, depending on the alternative hypothesis). Note that in doing so you can use a Z score, where $Z = \\frac{sample~statistic - null~value}{SE} = \\frac{\\bar{x} - \\mu_0}{SE}$  Always sketch the normal curve when calculating the p-value, and shade the appropriate area(s) depending on whether the alternative hypothesis is one- or two-sided.   Infer that if a confidence interval does not contain the null value the null hypothesis should be rejected in favor of the alternative. Compare the p-value to the significance level to make a decision between the hypotheses:  If the p-value $\u0026lt;$ the significance level, reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is extremely unlikely to happen just by chance, and conclude that the data provides evidence for the alternative hypothesis. If the p-value $\u0026gt;$ the significance level, fail to reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is quite likely to happen by chance, and conclude that the data does not provide evidence for the alternative hypothesis. Note that we can never \u0026ldquo;accept\u0026rdquo; the null hypothesis since the hypothesis testing framework does not allow us to confirm it.   Note that the conclusion of a hypothesis test might be erroneous regardless of the decision we make.  Define a Type 1 error as rejecting the null hypothesis when the null hypothesis is actually true. Define a Type 2 error as failing to reject the null hypothesis when the alternative hypothesis is actually true.   Choose a significance level depending on the risks associated with Type 1 and Type 2 errors.  Use a smaller $\\alpha$ is Type 1 error is relatively riskier. Use a larger $\\alpha$ is Type 2 error is relatively riskier.   Formulate the framework for statistical inference using hypothesis testing and nearly normal point estimates:  Set up the hypotheses first in plain language and then using appropriate notation. Identify the appropriate sample statistic that can be used as a point estimate for the parameter of interest. Verify that the conditions for the CLT holds. Compute the SE, sketch the sampling distribution, and shade area(s) representing the p-value. Using the sketch and the normal model, calculate the p-value and determine if the null hypothesis should be rejected or not, and state your conclusion in context of the data and the research question.   If the conditions necessary for the CLT to hold are not met, note this and do not go forward with the analysis. (We will later learn about methods to use in these situations.) Distinguish statistical significance vs. practical significance.  Supplemental Readings   OpenIntro Statistics slides\n  Why do we use 0.05 as a significance level?\n  Videos     "
},
{
	"uri": "/chapters/",
	"title": "Chapters",
	"tags": [],
	"description": "",
	"content": " Chapter 1\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Introduction to Data Learning Objectives Identify the type of variables (e.g. numerical or categorical; discrete or continuous; ordered or not ordered). Identify the relationship between multiple variables (i.e. independent vs. dependent). Define variables that are not associated as independent.\n  Chapter 2\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Summarizing Data Learning Outcomes Use appropriate visualizations for different types of data (e.g. histogram, barplot, scatterplot, boxplot, etc.). Use different measures of center and spread and be able to describe the robustness of different statistics. Describe the shape of distributions vis-a-vis histograms and boxplots.\n  Chapter 3\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Probability Learning Outcomes Define trial, outcome, and sample space. Define and describe the law of large numbers. Distinguish disjoint (also called mutually exclusive) and independent events. Use Venn diagrams to represent events and their probabilities. Describe probability distributions.\n  Chapter 4\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Distributions of Random Variables Learning Outcomes Define the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: $Z = \\frac{x - \\mu}{\\sigma}$. Use the Z score if the distribution is normal: to determine the percentile score of a data point (using technology or normal probability tables) regardless of the shape of the distribution: to assess whether or not the particular observation is considered to be unusual (more than 2 standard deviations away from the mean) Depending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score.\n  Chapter 5\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Foundations for Inference Learning Outcomes Define sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous.\n  Chapter 6\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Inference for Categorical Data Learning Outcomes Define population proportion $p$ (parameter) and sample proportion $\\hat{p}$ (point estimate). Calculate the sampling variability of the proportion, the standard error, as [ SE = \\sqrt{\\frac{p(1-p)}{n}}, ] where $p$ is the population proportion.\n  Chapter 7\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Inference for Numerical Data Learning Outcomes Use the $t$-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means. Explain why the $t$-distribution helps make up for the additional variability introduced by using $s$ (sample standard deviation) in calculation of the standard error, in place of $\\sigma$ (population standard deviation).\n  Chapter 8\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Introduction to Linear Regression Learning Outcomes Define the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted). Plot the explanatory variable ($x$) on the x-axis and the response variable ($y$) on the y-axis, and fit a linear regression model $y = \\beta_0 + \\beta_1 x$ where $\\beta_0$ is the intercept, and $\\beta_1$ is the slope.\n  Chapter 9\nMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } }); Multiple and Logistic Regression Learning Outcomes Define the multiple linear regression model as $$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k$$ where there are $k$ predictors (explanatory variables). Interpret the estimate for the intercept ($b_0$) as the expected value of $y$ when all predictors are equal to 0, on average.\n  Bayesian\nBayesian Analysis Supplemental Readings Chapter 17 of Learning Statistics with R (Navarro, version 0.6) Fitting a Model by Maximum Likelihood (Collier, 2013). Kruschke\u0026rsquo;s website for Doing Bayesian Data Analysis Kruschke\u0026rsquo;s blog Andrew Gelman\u0026rsquo;s blog - Posts about bayesian statistics Videos Rasmus Bååth\u0026rsquo;s Introduction to Bayesian Data Analysis Video Series John Kruschke\u0026rsquo;s Video Series Bayesian Methods Interpret Data Better Bayesian Estimation Supersedes the t Test\n  "
},
{
	"uri": "/course-overview/links/",
	"title": "Links",
	"tags": [],
	"description": "",
	"content": "These are some useful resources on the web for learning R. Feel free to suggest other resources by clicking the \u0026ldquo;Improve this page\u0026rdquo; button in the top right.\nLearning R  R for Data Science. Book by Garrett Grolemund and Hadley Wickham Quick-R. Kabakoff\u0026rsquo;s website. Great reference along with his book, R in Action. O\u0026rsquo;Reilly Try R. Great tutorial on R where you can try R commands directly from the web browser. R Reference Card Video Overview of RStudio R-Bloggers Journal of Statistical Software The R Journal An Introduction to Statistical Learning with Applications in R  Learning R Markdown  Video on RMarkdown by RStudio - This 26 minute video talks about some updates to RMarkdown. Markdown Basics. Markdown is a way of formatting plain text documents mostly for the web. However, it has become for other writing tasks too. It has become popular because it focusses on writing and not formatting. The formatting is taken care later. The Markdown Basics provides a nice introduction to Markdown. The R Markdown Website has a nice introduction on how Markdown is extended to allow for the inclusion of R code and output. Video Introduction to R Markdown. This short video (under 4 minutes) was recorded with an older version, so not all of the features and dialog boxes will look the same, but may be helpful.  Websites  R-Bloggers  Podcasts  No So Standard Deviations @nssdeviations  People Data scientists, statisticians, and generally interesting people.\n Hadley Wickham @hadleywickham Chris Albon @chrisalbon David Smith @revodavid Alex Hayes @alexpghayes Max Kuhn @topepos Daniela Witten @daniela_witten Dianne Cook @visnut Mara Averick @dataandme Angela Bassa @AngeBassa Julia Silge @juliasilge Frank Harrell @f2harrell Joe Cheng @jcheng Amelia McNamara @AmeliaMN Jenny Bryan @JennyBryan Elizabeth Stuart @lizstuartdc David Robinson @drob Gary King @kinggary Rasmus Bååth @rabaath Mine CetinkayaRundel @minebocek Karl Browman @kwbroman Andrew Gelman @statmodeling  Videos  Understanding PCA using Shiny and Stack Overflow data  "
},
{
	"uri": "/assignments/project/",
	"title": "Project",
	"tags": [],
	"description": "",
	"content": "  Download project proposal template Download project template  The purpose of the data project is for you to conduct a reproducible analysis with a data set of your choosing. There are two components to the project, the proposal, which will be graded on a pass/fail basis, and the final report. The outline for each of these are provided in the templates. When submitting the assignments, include the R Markdown file (change the name to include your last name, for example Bryer-Proposal.Rmd and Bryer-Project.Rmd) along with any supplementary files necessary to run the R Markdown file (e.g. data files, screenshots, etc.). Suggestions for possible data sources are included below, however you are free to use data not listed below. The only requirement is that you are allowed to share the data. Projects will be shared with others on this website so should be presented in a way that other students can reproduce your analysis.\nProject Proposal The proposal can be more informal using bullet points where necessary and include R code and output. You must address the following areas:\n Research question What are the cases, and how many are there? Describe the method of data collection. What type of study is this (observational/experiment)? Data Source: If you collected the data, state self-collected. If not, provide a citation/link. Response: What is the response variable, and what type is it (numerical/categorical)? Explanatory: What is the explanatory variable(s), and what type is it (numerical/categorival)? Relevant summary statistics  Example data project proposal (Source Rmarkdown file)\n Final Project  See the November 4th meetup for more information on the data project including an example presentation. Click here for the slides. You are required to attend ONLY ONE of those time slots. You will do your presentation, watch the other presentations, and provide peer feedback (will be shared anonymously afterward).  Click here to sign-up for a presentation time.\nChecklist / Suggested Outline  Overview slide  Context on the data collection Description of the dependent variable (what is being measured) Description of the independent variable (what is being measured; include at least 2 variables) Research question  Summary statistics Include appropriate data visualizations. Statistical output  Include the appropriate statistics for your method used. For null hypothesis tests (e.g. t-test, chi-squared, ANOVA, etc.), state the null and alternative hypotheses along with relevant statistic and p-value (and confidence interval if appropriate). For regression models, include the regression output and interpret the R-squared value.  Conclusion  Why is this analysis important? Limitations of the analysis?    Rubric     Domain Accomplished Proficient Needs Improvement    Introduction The research question is clearly stated, can be answered by the data, and the context of the problem clearly explained. The research question is unclear and/or not supported by the data. Research question is ambiguous, unclear, or not stated.  Data Display Includes appropriate, well-labeled, accurate displays (graphs and tables) of the data. Includes appropriate, accurate displays of the data. Includes appropriate but no accurate displays of the data.  Data Analysis The appropriate statistical test(s) was used for the data and interpretation was clear. The appropriate statistical test(s) was used but interpretation was not fully clear or well articulated. The incorrect statistical test was used an/or not justified for the data as presented.  Conclusion Conclusion includes a clear answer to the statistical question that is consistent with the data analysis and the method of data collection. Conclusion includes an answer to the statistical question that is consistent with the data but not with the data collection method. Conclusion does not include an answer to the statistical question that is consistent with the data analysis.  Overall Presentation Attractive, well-organized, well-written presentation Presentation has two of the three qualities: attractive, well-organized, well-written. Presentation is not attractive, organized, or written. There are numerous errors throughout.      Example Data Sources You are not to use data sources used in class or the textbooks. Possible data sources include, but are not limited to:\n FiveThirtyEight https://github.com/fivethirtyeight/data RStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/ Analyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/ The World Bank Data Catalog http://datacatalog.worldbank.org/ Google Public Data search engine http://www.google.com/publicdata/directory Vanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets Programme of International Student Assessment (PISA) http://www.oecd.org/pisa/ Behavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/ World Values Survey http://www.worldvaluessurvey.org/wvs.jsp American National Election Survey (ANES) http://www.electionstudies.org/ General Social Survey (GSS) http://www3.norc.org/GSS+Website/ Integrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/ U.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html   "
},
{
	"uri": "/chapters/chapter6/",
	"title": "Chapter 6",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Inference for Categorical Data Learning Outcomes  Define population proportion $p$ (parameter) and sample proportion $\\hat{p}$ (point estimate). Calculate the sampling variability of the proportion, the standard error, as [ SE = \\sqrt{\\frac{p(1-p)}{n}}, ] where $p$ is the population proportion.  Note that when the population proportion $p$ is not known (almost always), this can be estimated using the sample proportion, $SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$.   Recognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.  In the case of the proportion the CLT tells us that if \\  the observations in the sample are independent, \\ the sample size is sufficiently large (checked using the success/failure condition: $np \\ge 10$ and $n(1-p) \\ge 10$), \\\nthen the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of $\\sqrt{\\frac{p(1-p)}{n}}$. [ \\hat{p} \\sim N \\left( mean = p, SE = \\sqrt{\\frac{p(1-p)}{n}} \\right) ]     Note that if the CLT doesn?t apply and the sample proportion is low (close to 0) the sampling distribution will likely be right skewed, if the sample proportion is high (close to 1) the sampling distribution will likely be left skewed. Remember that confidence intervals are calculated as [ \\text{point estimate} \\pm \\text{margin of error} ] and test statistics are calculated as [ \\text{test statistic =} \\frac{\\text{point estimate - null value}}{\\text{standard error}} ] Note that the standard error calculation for the confidence interval and the hypothesis test are different when dealing with proportions, since in the hypothesis test we need to assume that the null hypothesis is true \u0026ndash; remember: p-value = P(observed or more extreme test statistic $|$ $H_0$ true).  For confidence intervals use $\\hat{p}$ (observed sample proportion) when calculating the standard error and when checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$ For hypothesis tests use $p_0$ (null value) when calculating the standard error and checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{p_0 (1-p_0)}{n}}$ Such a discrepancy doesn\u0026rsquo;t exist when conducting inference for means, since the mean doesn\u0026rsquo;t factor into the calculation of the standard error, while the proportion does.   Calculate the required minimum sample size for a given margin of error at a given confidence level, and explain why we use $\\hat{p} = 0.5$ if there are no previous studies suggesting a more accurate estimate.  Conceptually: When there is no additional information, 50% chance of success is a good guess for events with only two outcomes (success or failure). Mathematically: Using $\\hat{p} = 0.5$ yields the most conservative (highest) estimate for the required sample size.   Note that the calculation of the standard error of the distribution of the difference in two independent sample proportions is different for a confidence interval and a hypothesis test.  confidence interval (and hypothesis test when $H_0: p_1 -p_2 =$ some value other than 0): $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_1 (1 - \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1 - \\hat{p}_2)}{n_2} }$ hypothesis test when $H_0: p_1 -p_2 = 0$: $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_1} + \\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_2} }$, where $\\hat{p}_{pool}$ is the overall rate of success: $\\hat{p}_{pool} = \\frac{\\text{number of successes in group 1 + number of successes in group 2}}{n_1 + n_2}$   Note that the reason for the difference in calculations of standard error is the same as in the case of the single proportion: when the null hypothesis claims that the two population proportions are equal, we need to take that into consideration when calculating the standard error for the hypothesis test, and use a common proportion for both samples. Use a chi-square test of goodness of fit to evaluate if the distribution of levels of a single categorical variable follows a hypothesized distribution.  $H_0:$ The distribution of the variable follows the hypothesized distribution, and any observed differences are due to chance. $H_A:$ The distribution of the variable does not follow the hypothesized distribution.   Calculate the expected counts for a given level (cell) in a one-way table as the sample size times the hypothesized proportion for that level. Calculate the chi-square test statistic as $\\chi = \\sum_{i = 1}^{k} \\frac{(\\text{observed count} - \\text{expected count})^2}{\\text{expected count}}$, where $k$ is the number of cells. Note that the chi-square distribution is right skewed with one parameter: degrees of freedom. In the case of a goodness of fit test, $df = # \\text{of categories} - 1$. List the conditions necessary for performing a chi-square test (goodness of fit or independence)  the observations should be independent expected counts for each cell should be at least 5 degrees of freedom should be at least 2 (if not, use methods for evaluating proportions)   Describe how to use the chi-square table to obtain a p-value. When evaluating the independence of two categorical variables where at least one has more than two levels, use a chi-square test of independence.  $H_0:$ The two variables are independent. $H_A:$ The two variables are dependent.   Calculate expected counts in two-way tables as [ E = \\frac{\\text{row total} \\times \\text{column total}}{\\text{grand total}} ] Calculate the degrees of freedom for chi-square test of independence as $df = (R - 1) \\times (C - 1)$, where $R$ is the number of rows in a two-way table, and $C$ is the number of columns. Note that there is no such thing as a chi-square confidence interval for proportions, since in the case of a categorical variables with many levels, there isn\u0026rsquo;t one parameter to estimate. Use simulation methods when sample size conditions aren\u0026rsquo;t met for inference for categorical variables.  Note that the $t$-distribution is only appropriate to use for means, when sample size isn\u0026rsquo;t sufficiently large, and the parameter of interest is a proportion or a difference between two proportions, we need to use simulation.   In hypothesis testing  for one categorical variable, generate simulated samples based on the null hypothesis, and then calculate the number of samples that are at least as extreme as the observed data. for two categorical variables, use a randomization test.   Use bootstrap methods for confidence intervals for categorical variables with at most two levels.  Supplemental Readings  OpenIntro Statistics slides  Videos    "
},
{
	"uri": "/assignments/final/",
	"title": "Final Exam",
	"tags": [],
	"description": "",
	"content": "The final exam will be posted on Blackboard during the time indicated on the course schedule.\n"
},
{
	"uri": "/course-overview/mathjax/",
	"title": "Math Equations",
	"tags": [],
	"description": "",
	"content": " Occasionally you will need to type equations in homework and labs. R Markdown supports LaTeX style equations using the MathJax javascript library. I do not expect you to learn LaTeX for this course. Instead, I recommend using the free application Daum Equation Editor. It availabe online, as a Google Chrome Extension, or as a standalone Mac Application.\nCreating Equations with Daum Equation Editor Occasionally you will need to type equations in homework and labs. R Markdown supports LaTeX style equations using the MathJax javascript library. I do not expect you to learn LaTeX for this course. Instead, I recommend using the free application Daum Equation Editor. It availabe online, as a Google Chrome Extension, or as a standalone Mac Application. With the editor, you can enter quations using menus.\nOnce done, copy and paste the LaTeX code at the bottom into your R Markdown file between two dollar signs (i.e. $), and the quation will be rendered by the web browser.\n$$ f\\left( x|\\mu ,\\sigma \\right) =\\frac { 1 }{ \\sigma \\sqrt { 2\\pi } } { e }^{ -\\frac { { \\left( x-\\mu \\right) }^{ 2 } }{ { 2\\sigma }^{ 2 } } } $$\n"
},
{
	"uri": "/chapters/chapter7/",
	"title": "Chapter 7",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Inference for Numerical Data Learning Outcomes  Use the $t$-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means. Explain why the $t$-distribution helps make up for the additional variability introduced by using $s$ (sample standard deviation) in calculation of the standard error, in place of $\\sigma$ (population standard deviation). Describe how the $t$-distribution is different from the normal distribution, and what ?heavy tail? means in this context. Note that the $t$-distribution has a single parameter, degrees of freedom, and as the degrees of freedom increases this distribution approaches the normal distribution. Use a $t$-statistic, with degrees of freedom $df = n - 1$ for inference for a population mean:  Standard error: $SE = \\frac{s}{\\sqrt{n}}$ Confidence interval: $\\bar{x} \\pm t_{df}^\\star SE$ Hypothesis test: $T_{df} = \\frac{\\bar{x} - \\mu}{SE}$ \\end{itemize}   Describe how to obtain a p-value for a $t$-test and a critical $t$-score ($t^\\star_{df}$) for a confidence interval. Define observations as paired if each observation in one dataset has a special correspondence or connection with exactly one observation in the other data set. Carry out inference for paired data by first subtracting the paired observations from each other, and then treating the set of differences as a new numerical variable on which to do inference (such as a confidence interval or hypothesis test for the average difference). Calculate the standard error of the difference between means of two paired (dependent) samples as $SE = \\frac{s_{diff}}{\\sqrt{n_{diff}}}$ and use this standard error in hypothesis testing and confidence intervals comparing means of paired (dependent) groups. Use a $t$-statistic, with degrees of freedom $df = n_{diff} - 1$ for inference for a population mean: \\begin{itemize}  Standard error: $SE = \\frac{s}{\\sqrt{n}}$ Confidence interval: $\\bar{x}_{diff} \\pm t_{df}^\\star SE$ Hypothesis test: $T_{df} = \\frac{\\bar{x}_{diff} - \\mu_{diff}}{SE}$. Note that $\\mu_{diff}$ is often 0, since often $H_0: \\mu_{diff} = 0$.   Recognize that a good interpretation of a confidence interval for the difference between two parameters includes a comparative statement (mentioning which group has the larger parameter). Recognize that a confidence interval for the difference between two parameters that doesn?t include 0 is in agreement with a hypothesis test where the null hypothesis that sets the two parameters equal to each other is rejected. Calculate the standard error of the difference between means of two independent samples as $SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$, and use this standard error in hypothesis testing and confidence intervals comparing means of independent groups. Use a $t$-statistic, with degrees of freedom $df = min(n_1 - 1, n_2 - 1)$ for inference for a population mean:  Standard error: $\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$ Confidence interval: $(\\bar{x}_1 - \\bar{x}_2) \\pm t_{df}^\\star SE$ Hypothesis test: $T_{df} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{SE}$. Note that $\\mu_{diff}$ is often 0, since often $H_0: \\mu_1 - \\mu_2 = 0$.   Calculate the power of a test for a given effect size and significance level in two steps: (1) Find the cutoff for the sample statistic that will allow the null hypothesis to be rejected at the given significance level, (2) Calculate the probability of obtaining that sample statistic given the effect size. Explain how power changes for changes in effect size, sample size, significance level, and standard error. Define analysis of variance (ANOVA) as a statistical inference method that is used to determine if the variability in the sample means is so large that it seems unlikely to be from chance alone by simultaneously considering many groups at once. Recognize that the null hypothesis in ANOVA sets all means equal to each other, and the alternative hypothesis suggest that at least one mean is different. \\begin{itemize}  $H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k$ $H_A:$ At least one mean is different   List the conditions necessary for performing ANOVA  the observations should be independent within and across groups the data within each group are nearly normal the variability across the groups is about equal\nand check if they are met using graphical diagnostics.   Recognize that the test statistic for ANOVA, the F statistic, is calculated as the ratio of the mean square between groups (MSG, variability between groups) and mean square error (MSE, variability within errors), and has two degrees of freedom, one for the numerator ($df_{G} = k - 1$, where $k$ is the number of groups) and one for the denominator ($df_{E} = n - k$, where $n$ is the total sample size).  Note that you won\u0026rsquo;t be expected to calculate MSG or MSE from the raw data, but you should have a conceptual understanding of how they\u0026rsquo;re calculated and what they measure.   Describe why calculation of the p-value for ANOVA is always \u0026ldquo;one sided\u0026rdquo;. Describe why conducting many $t$-tests for differences between each pair of means leads to an increased Type 1 Error rate, and we use a corrected significance level (Bonferroni corection, $\\alpha^\\star = \\alpha / K$, where $K$ is the e number of comparisons being considered) to combat inflating this error rate. Describe why it is possible to reject the null hypothesis in ANOVA but not find significant differences between groups as a result of pairwise comparisons.  Supplemental Readings  OpenIntro Statistics slides  Videos         "
},
{
	"uri": "/chapters/chapter8/",
	"title": "Chapter 8",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Introduction to Linear Regression Learning Outcomes  Define the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted). Plot the explanatory variable ($x$) on the x-axis and the response variable ($y$) on the y-axis, and fit a linear regression model $y = \\beta_0 + \\beta_1 x$ where $\\beta_0$ is the intercept, and $\\beta_1$ is the slope.  Note that the point estimates (estimated from observed data) for $\\beta_0$ and $\\beta_1$ are $b_0$ and $b_1$, respectively.   When describing the association between two numerical variables, evaluate  direction: positive ($x \\uparrow, y \\uparrow$), negative ($x \\downarrow, y \\uparrow$) form: linear or not strength: determined by the scatter around the underlying relationship   Define correlation as the \\emph{linear} association between two numerical variables.  Note that a relationship that is nonlinear is simply called an association.   Note that correlation coefficient ($r$, also called Pearson\u0026rsquo;s $r$) the following properties:  the magnitude (absolute value) of the correlation coefficient measures the strength of the linear association between two numerical variables the sign of the correlation coefficient indicates the direction of association the correlation coefficient is always between -1 and 1, inclusive, with -1 indicating perfect negative linear association, +1 indicating perfect positive linear association, and 0 indicating no \\emph{linear} relationship the correlation coefficient is unitless since the correlation coefficient is unitless, it is not affected by changes in the center or scale of either variable (such as unit conversions) the correlation of X with Y is the same as of Y with X the correlation coefficient is sensitive to outliers   Recall that correlation does not imply causation. Define residual ($e$) as the difference between the observed ($y$) and predicted ($\\hat{y}$) values of the response variable. $e_i = y_i - \\hat{y}_i$ Define the least squares line as the line that minimizes the sum of the squared residuals, and list conditions necessary for fitting such line:  linearity nearly normal residuals constant variability   Define an indicator variable as a binary explanatory variable (with two levels). Calculate the estimate for the slope ($b_1$) as $b_1 = R\\frac{s_y}{s_x}$, where $r$ is the correlation coefficient, $s_y$ is the standard deviation of the response variable, and $s_x$ is the standard deviation of the explanatory variable. Interpret the slope as  \u0026ldquo;For each unit increase in $x$, we would expect $y$ to increase/decrease on average by $|b_1|$ units\u0026rdquo; when $x$ is numerical. \u0026ldquo;The average increase/decrease in the response variable when between the baseline level and the other level of the explanatory variable is $|b_1|$.\u0026rdquo; when $x$ is categorical. Note that whether the response variable increases or decreases is determined by the sign of $b_1$.   Note that the least squares line always passes through the average of the response and explanatory variables ($\\bar{x},\\bar{y}$). Use the above property to calculate the estimate for the slope ($b_0$) as $b_0 = \\bar{y} - b_1 \\bar{x}$, where $b_1$ is the slope, $\\bar{y}$ is the average of the response variable, and $\\bar{x}$ is the average of explanatory variable. Interpret the intercept as  \u0026ldquo;When $x = 0$, we would expect $y$ to equal, on average, $b_0$.\u0026rdquo; when $x$ is numerical. \u0026ldquo;The expected average value of the response variable for the reference level of the explanatory variable is $b_0$.\u0026rdquo; when $x$ is categorical.   Predict the value of the response variable for a given value of the explanatory variable, $x^\\star$, by plugging in $x^\\star$ in the in the linear model: $\\hat{y} = b_0 + b_1 x^\\star$  Only predict for values of $x^\\star$ that are in the range of the observed data. Do not extrapolate beyond the range of the data, unless you are confident that the linear pattern continues.   Define $R^2$ as the percentage of the variability in the response variable explained by the the explanatory variable.  For a good model, we would like this number to be as close to 100% as possible. This value is calculated as the square of the correlation coefficient, and is between 0 and 1, inclusive.   Define a leverage point as a point that lies away from the center of the data in the horizontal direction. Define an influential point as a point that influences (changes) the slope of the regression line.  This is usually a leverage point that is away from the trajectory of the rest of the data.   Do not remove outliers from an analysis without good reason. Be cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points. Determine whether an explanatory variable is a significant predictor for the response variable using the $t$-test and the associated p-value in the regression output. Set the null hypothesis testing for the significance of the predictor as $H_0: \\beta_1 = 0$, and recognize that the standard software output yields the p-value for the two-sided alternative hypothesis.  Note that $\\beta_1 = 0$ means the regression line is horizontal, hence suggesting that there is no relationship between the explanatory and the response variables.   Calculate the T score for the hypothesis test as $T_{df}=\\frac { b_{ 1 }-{ null\\quad value } }{ SE_{ b_{ 1 } } }$ with $df = n - 2$.  Note that the T score has $n - 2$ degrees of freedom since we lose one degree of freedom for each parameter we estimate, and in this case we estimate the intercept and the slope.   Note that a hypothesis test for the intercept is often irrelevant since it\u0026rsquo;s usually out of the range of the data, and hence it is usually an extrapolation. Calculate a confidence interval for the slope as $b_1 \\pm t^\\star_{df} SE_{b_1}$ where $df = n - 2$ and $t^\\star_{df}$ is the critical score associated with the given confidence level at the desired degrees of freedom.  Note that the standard error of the slope estimate $SE_{b_1}$ can be found on the regression output.    Supplemental Readings   OpenIntro Statistics slides\n  Interaction Terms\n  Regerssion for non-linear terms\n  Linear regression with SAT scores - This document outlines the implementation of linear regression step-by-step emphasizing visualizations.\n  Videos     "
},
{
	"uri": "/chapters/chapter9/",
	"title": "Chapter 9",
	"tags": [],
	"description": "",
	"content": "MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });   Multiple and Logistic Regression Learning Outcomes  Define the multiple linear regression model as $$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k$$ where there are $k$ predictors (explanatory variables). Interpret the estimate for the intercept ($b_0$) as the expected value of $y$ when all predictors are equal to 0, on average. Interpret the estimate for a slope (say $b_1$) as \u0026ldquo;All else held constant, for each unit increase in $x_1$, we would expect $y$ to increase/decrease on average by $b_1$.\u0026rdquo; Define collinearity as a high correlation between two independent variables such that the two variables contribute redundant information to the model \u0026ndash; which is something we want to avoid in multiple linear regression. Note that $R^2$ will increase with each explanatory variable added to the model, regardless of whether or not the added variables is a meaningful predictor of the response variable. Therefore we use adjusted $R^2$, which applies a penalty for the number of predictors included in the model, to better assess the strength of a multiple linear regression model: $$R^2 = 1 - \\frac{Var(e_i) / (n - k - 1)}{Var(y_i) / (n - 1)}$$ where $Var(e_i)$ measures the variability of residuals ($SS_{Err}$), $Var(y_i)$ measures the total variability in observed $y$ ($SS_{Tot}$), $n$ is the number of cases and $k$ is the number of predictors.  Note that adjusted $R^2$ will only increase if the added variable has a meaningful contribution to the amount of explained variability in $y$, i.e. if the gains from adding the variable exceeds the penalty.   Define model selection as identifying the best model for predicting a given response variable. Note that we usually prefer simpler (parsimonious) models over more complicated ones. Define the full model as the model with all explanatory variables included as predictors. Note that the p-values associated with each predictor are conditional on other variables being included in the model, so they can be used to assess if a given predictor is significant, given that all others are in the model.  These p-values are calculated based on a $t$ distribution with $n - k - 1$ degrees of freedom. The same degrees of freedom can be used to construct a confidence interval for the slope parameter of each predictor: $$b_i \\pm t^\\star_{n - k - 1} SE_{b_i}$$   Stepwise model selection (backward or forward) can be done based based on adjusted $R^2$ (choose the model with higher adjusted $R^2$). The general idea behind backward-selection is to start with the full model and eliminate one variable at a time until the ideal model is reached. i. Start with the full model. ii. Refit all possible models omitting one variable at a time, and choose the model with the highest adjusted $R^2$. iii. Repeat until maximum possible adjusted $R^2$ is reached. The general idea behind forward-selection is to start with only one variable and adding one variable at a time until the ideal model is reached. i. Try all possible simple linear regression models predicting $y$ using one explanatory variable at a time. Choose the model with the highest adjusted $R^2$. ii. Try all possible models adding one more explanatory variable at a time, and choose the model with the highest adjusted $R^2$. iii. Repeat until maximum possible adjusted $R^2$ is reached. Adjusted $R^2$ method is more computationally intensive, but it is more reliable, since it doesn\u0026rsquo;t depend on an arbitrary significant level. List the conditions for multiple linear regression as  linear relationship between each (numerical) explanatory variable and the response - checked using scatterplots of $y$ vs. each $x$, and residuals plots of $residuals$ vs. each $x$ nearly normal residuals with mean 0 - checked using a normal probability plot and histogram of residuals constant variability of residuals - checked using residuals plots of $residuals$ vs. $\\hat{y}$, and $residuals$ vs. each $x$ independence of residuals (and hence observations) - checked using a scatterplot of $residuals$ vs. order of data collection (will reveal non-independence if data have time series structure)   Note that no model is perfect, but even imperfect models can be useful.  Supplemental Readings  OpenIntro Statistics slides  Videos      "
},
{
	"uri": "/chapters/bayesian/",
	"title": "Bayesian",
	"tags": [],
	"description": "",
	"content": "Bayesian Analysis Supplemental Readings  Chapter 17 of Learning Statistics with R (Navarro, version 0.6) Fitting a Model by Maximum Likelihood (Collier, 2013). Kruschke\u0026rsquo;s website for Doing Bayesian Data Analysis Kruschke\u0026rsquo;s blog Andrew Gelman\u0026rsquo;s blog - Posts about bayesian statistics  Videos Rasmus Bååth\u0026rsquo;s Introduction to Bayesian Data Analysis Video Series    John Kruschke\u0026rsquo;s Video Series Bayesian Methods Interpret Data Better\n Bayesian Estimation Supersedes the t Test\n Precision is the goal\n "
},
{
	"uri": "/course-overview/materials/",
	"title": "Materials",
	"tags": [],
	"description": "Materials",
	"content": " These are supplemental materials. Click to download.\n _gen  assets images  stat_extra_interaction_effects.pdf stat_extra_nonlinear_relationships.pdf stats_handout.pdf Textbooks  lsr-0.6.pdf os4.pdf ProbStatBook.pdf Supplemental  os2_extra_inference_guide.pdf os2_prob_tables.pdf os4_tablet.pdf    "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/2020-09-01-colors/",
	"title": "Colors",
	"tags": ["colors", "plots"],
	"description": "",
	"content": " Upon grading the labs I noticed there is interest in adding color to the plots. I understand and appreciate the desire to make your figures more aesthetically pleasing. I have used colors to make the figures fit the overall style of whatever publication they are going to be used in. However, my general advice is to only use color when it adds value. For example, coloring bars or points based upon a quantitative variable is often very helpful. I generally ask myself this when adding color: \"Does the addition of color enhance the interpretation of my data or is it distracting?\nIf you are to use color, choosing a color palette that uses color blind safe colors is important. Here is a article I found that provides a good introduction and principles: https://venngage.com/blog/color-blind-friendly-palette/\nAdditionally, there are palettes designed for different purposes (qualitative for quantitative variables). I have found [this] website a really useful resource for picking pallets. I also use Color Brewer very useful for picking colors. Many of the pallets are built into ggplot2 with the XXX_color_brewer function.\nSome important references for data visualization more broadly (i.e. not limited to use of colors):\n Edward Tufte’s Visual Display of Quantitative Information William Cleveland’s Visualizing Data and The Elements of Graphing Data Leland Wilkinson’s The Grammar of Graphics and Hadley Wickham’s ggplot2: Elegant Graphics for Data Analysis  "
},
{
	"uri": "/tags/colors/",
	"title": "colors",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/plots/",
	"title": "plots",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/r/",
	"title": "R",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/random-numbers/",
	"title": "random numbers",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/2020-08-31-random_numbers_and_seeds/",
	"title": "Random Numbers and Seeds in R",
	"tags": ["random numbers"],
	"description": "",
	"content": " To explore how R generates randome numbers, we will use the rnorm function. This function draws a random number from a normal distribution with a mean = 0 and standard deviation = 1 (though these can be changed with the mean and sd parameters). With n = 1 we will get two random numbers.\nrnorm(n = 1) ## [1] 1.733789 rnorm(n = 1) ## [1] -1.516682 Each time you run the command you will get a different number. The set.seed function will sets a seed to the random number generator so that each subsequent run will produce the same number.\nset.seed(2112); rnorm(n = 1) ## [1] 0.9243372 set.seed(2112); rnorm(n = 1) ## [1] 0.9243372 Setting a different seed results in a different number.\nset.seed(2113); rnorm(n = 1) ## [1] 0.5499032 What are seeds?\nComputers are actually bad at random events. However, there are good algorithms that mimic random processes. These algorithms work by starting with some initial value, a seed, and executing a complex algorithm that approximates randomozation. The seed is often set to the current time in miliseconds. To visualize the ramdom process, we will use the sample function to randomly select a number between 1 and 100. We will consider the ouput for the first 1,000 seeds.\nrandom_numbers \u0026lt;- integer(1000) for(i in seq_len(length(random_numbers))) { set.seed(i) random_numbers[i] \u0026lt;- sample(1:100, size = 1) } library(ggplot2) ggplot(data.frame(x = 1:100, y = random_numbers), aes(x = x, y = y)) + geom_point() + xlab(\u0026#39;Seed\u0026#39;) + ylab(\u0026#39;Random Number\u0026#39;) "
},
{
	"uri": "/post/2020-08-24-getting-started-with-r/",
	"title": "Getting Started with R",
	"tags": ["R Markdown", "working directories"],
	"description": "",
	"content": " To get started with the labs, you will need to install R and other supporting applications. Links to download all the software is on the Software page at http://data606.net/software which is available on the Course Overview menu. Once you are in RStudio, you will need to install a number of R packages using the following two commands:\ninstall.packages(c(\u0026#39;openintro\u0026#39;,\u0026#39;OIdata\u0026#39;,\u0026#39;devtools\u0026#39;,\u0026#39;tidyverse\u0026#39;, \u0026#39;ggplot2\u0026#39;, \u0026#39;psych\u0026#39;,\u0026#39;reshape2\u0026#39;,\u0026#39;knitr\u0026#39;,\u0026#39;markdown\u0026#39;,\u0026#39;shiny\u0026#39;,\u0026#39;R.rsp\u0026#39;, \u0026#39;fivethirtyeight\u0026#39;)) devtools::install_github(\u0026quot;jbryer/DATA606\u0026quot;) Note that this course used to be listed as IS 606, hence the different package name. The library command will load the package and the startLab function will:\nCreate a folder called Lab1 in the current working directory. Open the the RMarkdown file (note that the name should have your login name).  library(DATA606) startLab(\u0026#39;Lab1\u0026#39;) Note the Lab1 directory has been created.\nRStudio Files Panel\n If you click Lab0 that will take you into that folder where you will see the RMardkown file (.Rmd extension) along with all the supporting files. Under the More menu there is an option to Set As Working Directory, you will want to do this before working in the RMarkdown file.\nSetting Working Directory\n The console pane will always tell you what your current working directory is. Clicking the arrow icon will open that folder in the Files tab on the right. R will look for files and directories that do not have absolute paths (i.e. starting with C: on windows or / on Linux and Macs) relative to the working directory.\nWorking Directory\n "
},
{
	"uri": "/tags/r-markdown/",
	"title": "R Markdown",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/working-directories/",
	"title": "working directories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/annoucement/",
	"title": "Annoucement",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/2020-08-16-welcome-to-data606/",
	"title": "Welcome to DATA606!",
	"tags": ["Annoucement"],
	"description": "",
	"content": " Welcome to DATA606! My name is Dr. Jason Bryer and I will be your instructor for this semester. I am an Assistant Professor in the Data Science and Information Systems department at CUNY SPS. Couple of important notes as you get started:\n The course syllabus located here: https://fall2020.data606.net I will post all course materials there. Blackboard will be used primarily for submitting assignments. Join the Slack channel by clicking here. This will be our primary mode of communication. For private communications, you can either send private Slack messages or email me at jason.bryer@sps.cuny.edu. Introduce yourself in the Getting Acquainted forum on Blackboard. Once you go through the syllabus, try starting Lab 1 as soon as you can. This will require you to install R and RStudio and will help get you get acquainted with R. The best way to reach me is by Slack or email at jason.bryer@sps.cuny.edu. If you would prefer to talk on the phone or setup a Zoom session, it is best to suggest some times by email first. Our meetups will be on Wednesdays from 8:30pm to 9:30pm. I am looking forward to getting to know everyone and a fantastic semester! Good luck!  "
},
{
	"uri": "/",
	"title": "DATA606 - Fall 2020",
	"tags": [],
	"description": "",
	"content": " DATA606 - Fall 2020 Instructor: Jason Bryer, Ph.D.\nClass Meetup: Wednesday 8:30pm to 9:30pm\nOffice Hours: Mondays 8:30pm to 9:30pm; Fridays 11:00am to 12:00pm Schedule time to meet\nEmail: jason.bryer@sps.cuny.edu\nCourse Description This course covers basic techniques in probability and statistics that are important in the field of data analytics. Discrete probability models, sampling from infinite and finite populations, statistical distributions, basic Bayesian statistics, and non-parametric statistical techniques for categorical data are covered in this course. Each of these statistical concepts will be applied in a variety of real-world scenarios through the use of case studies and customized data sets.\nCourse Learning Outcomes: By then end of the course, students should be able to:\n Understand the foundations of probability theory and perform basic probability calculations. Build basic stochastic models for commonly encountered business problems. Model situations involving uncertainty using appropriate probability distributions and conditional techniques. Explore and summarize data using descriptive statistics. Test hypotheses using classical and modern computational techniques. Construct estimators and calculate intervals using classical and modern computational techniques. Perform basic Bayesian statistical techniques for estimation and testing hypotheses.  Program Learning Outcomes addressed by the course:  Business Understanding. Learn when probabilistic techniques apply to certain categories of business problems, discuss the sorts of solutions that are possible, and understand the limitations of these techniques. Foundational Math Skills. Explore and analyze data, build probabilistic and statistical models, construct estimators, and test hypotheses. Predictive Modeling. Learn foundational techniques that underlie predictive modeling algorithms, such as Naïve Bayes. Presentation. Complete and submit collaborative assignments using techniques from the course.  How is this course relevant for data analytics professionals? Probabilistic techniques are the foundation of many data science applications from data exploration and visualization to outlier analysis, stochastic modelling, and data mining algorithms. This course will ensure that students have a strong understanding of these foundations.\nGrading  DAACS (5%) Homework (20%) Labs (40%) Data Project (15%) Final exam (15%) Meetup Presentation (5%)  Grade Distribution    Quality of Performance Letter Grade Range % GPA     Excellent - work is of exceptional quality A 93 - 100 4   Excellent A- 90 - 92.9 3.7   Good - work is above average B+ 87 - 89.9 3.3   Satisfactory B 83 - 86.9 3   Below Average B- 80 - 82.9 2.7   Poor C+ 77 - 79.9 2.3   Poor C 70 - 76.9 2   Failure F \u0026lt; 70 0    How This Course Works: This course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. Most weeks will have homework assignments to be submitted. There will also be a presentation required and a forum post introduction required. You are expected to complete all assignments by their due dates.\nMeetup presentations will comprise the solution and presentation to the class of one of the suggested problems for study from the weekly materials (not the graded homework problems). Each student must present one problem throughout the semester. Problems are chosen by entering your name and problem in the Google Spreadsheet. Note there is a maximum of three presentations per meetup and presentations should be no more than five minutes. Additionally, prepare your presentation so that the slides or document (I suggest using R Markdown) will be shared on the course website. Problems are assigned first come, first served, so any problem not already chosen by another student is available.\nFurther details on each of these assignments will be available in Blackboard and/or this Github repository.\nTextbooks Required Diez, D.M., Barr, C.D., \u0026amp; Çetinkaya-Rundel, M. (2019). OpenIntro Statistics (4th Ed).\n This is an open source textbook and can be downloaded in PDF format here, from the OpenIntro website, or a printed copy can be ordered from Amazon.\n Navarro, D. (2018, version 0.6). Learning Statistics with R\n This is free textbook that supplements a lot of the material covered in Diez and Barr. We will use the chapter on Bayesian analysis. You can download a PDF version, Bookdown version, or visit the author\u0026rsquo;s website at learningstatisticswithr.com.\n Recommended Wickham, H., \u0026amp; Grolemund, G. (2016) R for Data Science. O\u0026rsquo;Reilly.\n Most of this books is available freely online at r4ds.had.co.nz/ but can be purchased from Amazon.\n Kabacoff, R.I. (2011). R in Action. Manning Publications.\n You can find a lot of the material in R in Action on Kabacoff\u0026rsquo;s website, statmethods.net. You can receive 38% off using the ria38 promo code when ordering from here.\n Wickham, H. Advanced R. Baca Raton, FL: Taylor \u0026amp; Francis Group.\n Most of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.\n Kruschke, J.K. (2014). Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (2nd Ed). London: Academic Press.\n This book can be purchased from Amazon, but also check out the author\u0026rsquo;s webiste (doingbayesiandataanalysis.blogspot.com/) for additional resources.\n Other Documents  Probability Tables Inference Guide RStudio Cheat Sheets Probability Cheat Sheet  Contact Office Hours (cell phone or using Zoom): By appointment. You\u0026rsquo;re encouraged to schedule an appointment, but you can try to call anytime.\nYou are encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 to 48 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message.\nI will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.\nAccessibility and Accommodations The CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\nOnline Etiquette and Anti-Harassment Policy The University strictly prohibits the use of University online resources or facilities, including Blackboard, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\nAcademic Integrity Academic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students' personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\nStudent Support Services If you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/\n"
},
{
	"uri": "/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "DATA606 - Fall 2020\n"
},
{
	"uri": "/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
}]